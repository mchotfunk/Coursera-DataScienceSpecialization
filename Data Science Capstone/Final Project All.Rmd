---
title: "Final"
author: "Andrew Abisha Hu"
date: "9/3/2018"
output: html_document
---
## Introduction

The goal here is to build your first simple model for the relationship between words. This is the first step in building a predictive text mining application. You will explore simple models and discover more complicated modeling techniques.

###  Task to accomplish

1. Build basic n-gram model - using the exploratory analysis you performed, build a basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words.

2. Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.


---


## Executive Summary

The objective of this milestone report is to explain the process of building the n-grams model in order to create a Natural Language Processing (NLP) App and present a basic exploratory analysis of the data set. 

I use the file that contains "English" language only, sources from blog, twitter and news. Because the original data file is too huge, I sample them with 25000 rows in each source (Twitter, Blog and News), and then combine them altogether to a file called "sample" (with total 75000 observations)

After create the sample file, I start cleaning the data, removing the unwanted symbols. Getting the data prepared, I run the sample data through the "Work Flow". That is: Corpus, Tokenization and create a Document Frequency Matrix (DFM). 

Finally, I did a exploratory analysis using the dfm just created, providing top 20 features in each dfm with ggplot.



Note: I use the "quanteda" package instead of the classic text mining "tm" package becuase quanteda is simply more efficient.


## Data Processing

### Loading libraries
```{r loading}
suppressWarnings(library(stringi))
suppressWarnings(library(ggplot2))
suppressWarnings(library(quanteda))
suppressWarnings(library(dplyr))
suppressWarnings(library(data.table))
```

### Reading the data:

```{r }
#Set work directory
setwd("/users/andrewhu/desktop/final/en_US")
#Read text file
enblogrul <- file("en_US.blogs.txt", open="rb") 
entwurl <- file("en_US.twitter.txt", open="rb")
enneurl <- file("en_US.news.txt", open="rb")

#Read in the lines
en_blogs<- readLines(enblogrul, encoding = "UTF-8", skipNul=TRUE)

en_tw<- readLines(entwurl, encoding = "UTF-8", skipNul=TRUE)

en_news<- readLines(enneurl, encoding = "UTF-8", skipNul=TRUE)

close(enblogrul)
close(entwrul)
close(enneurl)

```

### Summaries of three files:

```{r}

# legnth of Lines for each file 
length(en_blogs) 
length(en_tw)  
length(en_news) 

# Numbers of words for each file
sum(stri_count_words(en_blogs)) 
sum(stri_count_words(en_news))  
sum(stri_count_words(en_news))

# max characters  for each file
max(nchar(en_blogs))
max(nchar(en_tw)) 
max(nchar(en_news))
```

---

### Sampling the data 

Simply sample each file of n rows, and then combine them.
```{r}
set.seed(525)
mini_tw <- sample(en_tw, size = 25000, replace = F)
mini_blogs <- sample(en_blogs, size = 25000, replace = F)
mini_news <- sample(en_news, size = 25000, replace = F)

sample <- c(mini_tw,mini_blogs,mini_news)

```

### Cleaning data
```{r}
# filter weird symbols
sample <- gsub("[^[:alpha:][:space:]']", " ", sample)
sample <- gsub("รฃ", "'", sample)
sample <- gsub("รฐ", "'", sample)

sample <- gsub(" #\\S*","", sample) #delete hastag
sample <- gsub("(f|ht)(tp)(s?)(://)(\\S*)", "", sample) # delete http links
sample <- gsub("[^0-9A-Za-z///' ]", "", sample) #delete special chars

#check the length for our sample
length(sample)

sample
```

----
## Work flow: 


### Corpus:

**Purpose:** 

```
 1. Save character strings and variables in a data frame
 
 2. Combine texts with document-level variables
 
```
```{r corpus}
#corpus for the whole sample
corp <- corpus(sample)
```


### Tokens:

**Purpose:**

```
Stores tokens in a list of vectors
```

```{r token}
#tokenization for the corpus
toks <- tokens(corp)

#generate ngrams for 1~3
unigram <- tokens_ngrams(toks, n=1)
bigram <- tokens_ngrams(toks, n=2)
trigram <- tokens_ngrams(toks, n=3)
```

### Document feature matrix (DFM)

**Purpose:**

```
Represents frequencies of features in documents in a matrix
```

```{r dfm }
#document frequency matrix for ngram 1~4
unigram_dfm <- dfm(unigram, what="fasterword", ngrams=1)

bigram_dfm <- dfm(bigram, what="fasterword", ngrams=2)

trigram_dfm <- dfm(trigram, what="fasterword", ngrams=3)
```


## Exploratory Analysis:

After processing the data, we can start creating top features for each n grams, subsetting them into different data frame and then plotting them to see the frequencies. 
```{r}
#Create top features for each n gram
top20_n1<- topfeatures(unigram_dfm,20)
top20_n2<- topfeatures(bigram_dfm ,20)
top20_n3<- topfeatures(trigram_dfm,20)

#create data frame 
top20_df_n1 <- data.frame(word=names(top20_n1), freq=top20_n1,row.names = NULL)
top20_df_n2 <- data.frame(word=names(top20_n2), freq=top20_n2,row.names = NULL)
top20_df_n3 <- data.frame(word=names(top20_n3), freq=top20_n3,row.names = NULL)

#Bar chart for ngram:1, top 20 features
g <- ggplot(top20_df_n1[1:20,], aes(reorder(word,-freq),freq)) 
g + geom_bar(stat="identity") + labs(x= "words", y= "freq")+coord_flip()

#Bar chart for ngram:2, top 20 features
g <- ggplot(top20_df_n2[1:20,], aes(reorder(word,-freq),freq)) 
g + geom_bar(stat="identity")+ labs(x= "words", y= "freq")+coord_flip()

#Bar chart for ngram:3, top 20 features
g <- ggplot(top20_df_n3[1:20,], aes(reorder(word,-freq),freq)) 
g + geom_bar(stat="identity") + labs(x= "words", y= "freq")+coord_flip()

plot(top20_df_n1[1:20,], max.words=100, colors = brewer.pal(6, "Dark2"), scale=c(8, .5))
title("Top 100 Twitter Words")
```





---

```{r}

#find the count of corpus

uni_sum <- colSums(unigram_dfm)
bi_sum <- colSums(bigram_dfm)
tri_sum <- colSums(trigram_dfm)


#Create data table for uni, bi and tri

uni_dt <- data.table(word_1 = names(uni_sum), count = uni_sum)

bi_dt <- data.table(
        word_1 = sapply(strsplit(names(bi_sum), "_", fixed = TRUE), '[[', 1),
        word_2 = sapply(strsplit(names(bi_sum), "_", fixed = TRUE), '[[', 2),
        count = bi_sum)

tri_dt <- data.table(
        word_1 = sapply(strsplit(names(tri_sum), "_", fixed = TRUE), '[[', 1),
        word_2 = sapply(strsplit(names(tri_sum), "_", fixed = TRUE), '[[', 2),
        word_3 = sapply(strsplit(names(tri_sum), "_", fixed = TRUE), '[[', 3),
        count = tri_sum)


#index 

setkey(uni_dt, word_1)
setkey(bi_dt, word_1, word_2)
setkey(tri_dt, word_1, word_2, word_3)


#Kneser-Kney Smoothing
discount_value <- 0.75

######## Bi-Gram Probability ###########

# number of words in Bi gram 
num_words_bigram <- nrow(bi_dt[by = .(word_1, word_2)])


# Calculating the probability: for a word "given the number of times it was the second word"
prob_bigram <- bi_dt[, .(Prob = ((.N) / num_words_bigram)), by = word_2]
setkey(prob_bigram, word_2)

# Assigning the previos probabilities (prob_bigram),  to unigrams
uni_dt[, Prob := prob_bigram[word_1, Prob]]
uni_dt <- uni_dt[!is.na(uni_dt$Prob)]


prob_ni <- bi_dt[, .(N = .N), by = word_1]
setkey(prob_ni, word_1)

# Assigning total times word 1 occured to bigram cn1
bi_dt[, Cn1 := uni_dt[word_1, count]]

# Kneser Kney Algorithm
bi_dt[, Prob := ((count - discount_value) / Cn1 + discount_value / Cn1 * prob_ni[word_1, N] * uni_dt[word_2, Prob])]


###################


##### Trigram probability ####


# Finding count of word1-word2 combination in bigram 
tri_dt[, Cn2 := bi_dt[.(word_1, word_2), count, allow.cartesian=T]]

# Finding count of word1-word2 combination in trigram
prob_trigram <- tri_dt[, .N, by = .(word_1, word_2)]
setkey(prob_trigram, word_1, word_2)

# Kneser Kney Algorithm
tri_dt[, Prob := (count - discount_value) / Cn2 + discount_value / Cn2 * prob_trigram[.(word_1, word_2), N] *
              bi_dt[.(word_1, word_2), Prob,allow.cartesian=T]]


# Finding the most frequently used 50 unigrmas
uni_dt <- uni_dt[order(-Prob)][1:50]
```


```{r save rds}
saveRDS(uni_dt, "uni_dt.rds")
saveRDS(bi_dt, "bi_dt.rds")
saveRDS(tri_dt, "tri_dt.rds")
```





```{r app }
# function to return highly probable previous word given two successive words
triWords <- function(w1, w2, n = 5) {
    pwords <- tri_dt[.(w1, w2)][order(-Prob)]
    if (any(is.na(pwords)))
        return(biWords(w2, n))
    if (nrow(pwords) > n)
        return(pwords[1:n, word_3])
    count <- nrow(pwords)
    bwords <- biWords(w2, n)[1:(n - count)]
    return(c(pwords[, word_3], bwords))
}



# function to return highly probable previous word given a word
biWords <- function(w1, n = 5) {
    pwords <- bi_dt[w1][order(-Prob)]
    if (any(is.na(pwords)))
        return(uniWords(n))
    if (nrow(pwords) > n)
        return(pwords[1:n, word_2])
    count <- nrow(pwords)
    unWords <- uniWords(n)[1:(n - count)]
    return(c(pwords[, word_2], unWords))
}

# function to return random words from unigrams
uniWords <- function(n = 5) {  
    return(sample(uni_dt[, word_1], size = n))
}



# The prediction app
getWords <- function(str){
    require(quanteda)
    tokens <- tokens(x = char_tolower(str))
    tokens <- char_wordstem(rev(rev(tokens[[1]])[1:2]), language = "english")
    
    words <- triWords(tokens[1], tokens[2], 5)
    chain_1 <- paste(tokens[1], tokens[2], words[1], sep = " ")

    print(words[1])
}

getWords("I am")

```
### Reference:

![]https://tutorials.quanteda.io/basic-operations/workflow/

![]https://github.com/Muhomorik/DSS_Capstone/blob/master/Helpers/ReadAndCleanFile.R

![]https://thiloshon.wordpress.com/2018/03/11/build-your-own-word-sentence-prediction-application-part-02/

![]https://rstudio-pubs-static.s3.amazonaws.com/95043_caa67c702dbc4e528d5bf3210e6c9f2b.html

