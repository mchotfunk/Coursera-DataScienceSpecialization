---
title: "Getting and Cleaning data-Notes"
output: html_document
author: Andrew Abisha Hu
---
## **Table of content**

* **Week1**

        * Read XML file

        * Read HTML file

        * Read json file
 
        * data.table (an alternative to create data frame)
        
* **Week2**

        * Connect mySQL
        
        * Reading HDF5
        
        * Reading data from web
        
* **Week3**

        * Subsetting and sorting data
        
        * Making tables
        
        * Checking missing values
        
        * Creating new variables
        
        * Manipulating data: dplyr
        
        * Merge data
        
* **Week4**

        * Editing test variables
        
        * Finding values in text (grasping pecific text content)
        

## **Reading internet files (XML, HTML, json)**
```{r setup, include=FALSE}
library(XML)
library(RCurl)

#set up url and download the file
fileUrl<- "http://www.w3schools.com/xml/simple.xml"
download.file(fileUrl, "/users/andrewhu/desktop/Coursera/simple.xml")
doc<- xmlTreeParse("/users/andrewhu/desktop/Coursera/simple.xml", useInternalNodes = TRUE) #wrapped up
rootNode<- xmlRoot(doc) #access to the element 
xmlName(rootNode) #get the name

#locate, reference
rootNode[[1]]
rootNode[[1]][[1]]

xmlSApply(rootNode,xmlValue) 

xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)

#Reading Html file

fileUrl2 <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
download.file(fileUrl2, "/users/andrewhu/desktop/Coursera/simple2.html")
doc2<- htmlTreeParse("/users/andrewhu/desktop/Coursera/simple2.html", useInternalNodes = TRUE)

scores<- xpathSApply(doc2,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc2,"//li[@class='team-name']", xmlValue)



library(jsonlite)
jsonData<- fromJSON("http://api.github.com/users/jtleek/repos")
names(jsonData)
names(jsonData$owner)

myjson <- toJSON(iris,pretty = TRUE)
cat(myjson)

iris2 <- fromJSON(myjson)
head(iris2)
```


## **data.table**
```{r}
#Data.table

#simply create a data table

DT= data.table(x=rnorm(9), y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)

#subsetting rows
DT[2:3,]
DT[c(2,3)] #these two are the same

DT[,c(2,3)] #all rows with columns 2 and 3

#Calculate values 
DT[,list(mean(x),sum(z))]

#Adding new columns
DT[,w:= z^2]

#Logical operations
DT[,a:= x>0]

#calculate mean, set by a

DT[,b:= mean(x+w), by=a]


#SetKeys: like a reference
DT <- data.table(x=rep(c("a","b","c"),each=100),y=rnorm(100))
setkey(DT,x)
DT['b']

###Fast read (reference from quiz)

#Read the table
housing <- data.table::fread ("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
#How many is val = 24 

housing[VAL==24, .N]
```

---


**Reading MySQL**
```{r , include=FALSE}
library(RMySQL)
ucscDb<- dbConnect(MySQL(),user="genome",host="genome-mysql.cse.ucsc.edu")
result<- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);

result
#connect
hg19<- dbConnect(MySQL(),user="genome",db="hg19",host="genome-mysql.cse.ucsc.edu")
allTables<- dbListTables(hg19)
length(allTables)

allTables[1:5]

dbGetQuery(hg19, "select count(*) from affyU133Plus2")

affyData<- dbReadTable(hg19,"affyU133Plus2")
head(affyData)

#select the subset of the data
query<- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")

affyMis <- fetch(query); quantile(affyMis$misMatches)

affyMisSmall <- fetch(query, n=10); dbClearResult(query) #remember to clear 

dim(affyMisSmall)

affyMisSmall

#close the connection
dbDisconnect(hg19)
```


**Reading HDF5**
```{r , include=FALSE}
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")

library(rhdf5)
created= h5createFile("example.h5")
created
```

**Reading data from the web**
```{r , include=FALSE}
#Read the url
con = url("http://scholar.google.com/citations?users=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode


#Parsing with XML
library(XML)
url <- "http://scholar.google.com/citations?users=HI-I6C0AAAAJ&hl=en"

html <- htmlTreeParse(url,useInternalNodes = TRUE)
xpathSApply(html,"//title", xmlValue)


#Get from httr package
library(httr); html2= GET(url)
content2 = content(html2,as ="text")
parsedHtml = htmlParse(content2,asText= TRUE)
xpathSApply(parsedHtml,"//title", xmlValue)

```

## **Subsetting and sorting**
```{r }
#Warm up
set.seed(13435)
X<- data.frame("var1"= sample(1:5),"var2"= sample(6:10),"var3"= sample(11:15))
X<- X[sample(1:5),] ; X$var2[c(1,3)]=NA
X

X[,1]
X[,"var1"]
X[["var1"]]

X[1:2, "var2"]

#subset with logical 
X[X$var1 <= 3 & X$var3 >11,]
X[X$var1 <= 3 | X$var3 >15,]


#Dealing with missing values: Use which

X[which(X$var2>8), ] #no  NA
X[X$var2>8, ]# with NA


#Sorting
sort(X$var1)
sort(X$var1,decreasing = TRUE)
sort(X$var2,na.last = TRUE)
X[order(X$var1),]

#ordering with plyr
library(plyr)

arrange(X,var1)
arrange(X,desc(var1))


#adding rows and cols
X$var4 <- rnorm(5)
X
Y<- cbind(X, rnorm(5))
Y
```

## **Summarizing data**
```{r}
rest<- read.csv("/users/andrewhu/Documents/GitHub/Coursera-DataScienceSpecialization/03_Getting and Cleaning Data/Data/rest.csv")
#head(data)
#tail(data)
#str(data)
#quantile(data$var, na.rm=TRUE)
#quantile(data$var, probs=c(0.5,0.7,0.9))


#make a table
table(rest$zipCode, useNA="ifany") #not missing the NAs (will include NAs)

#two dimensional table
table(rest$councilDistrict, rest$zipCode)

#check for missing values

sum(is.na(rest$councilDistrict))

any(is.na(rest$councilDistrict))

colSums(is.na(rest))

all(colSums(is.na(rest))==0)


#finding values with specific characteristics
table(rest$zipCode %in% c("21212","21213"))


#use this logical var to subset

rest[rest$zipCode %in% c("21212","21213"),]

#Cross tabs
data(UCBAdmissions)
DF= as.data.frame(UCBAdmissions)
summary(DF)

x1<- xtabs(Freq~Gender+Admit, data=DF)
x1


#Flat tables
warpbreaks$replicate <- rep(1:9,len=54)
xt= xtabs(breaks~., data=warpbreaks)
xt

ftable(xt)
```


## **Creating new variables**
```{r}
#Create sequences: need an index for data set
s1<- seq(1,10,by=2) ; s1 #specify the interval
s2<- seq(1,10,length=3) ; s2 #specify the length
x<- c(1,3,8,25,100); seq(along=x) #create index for the 5 values in x

#subsetting variables
rest$nearme = rest$neighborhood %in% c("Roland Park", "Homeland")
table(rest$nearme)

#Create binary variables
rest$zipWrong = ifelse(rest$zipCode<0, TRUE, FALSE) #if <0 then true, otherwise false.
table(rest$zipWrong, rest$zipCode<0)


#Create categorical variables
rest$zipGroups = cut(rest$zipCode, breaks= quantile(rest$zipCode))

table(rest$zipGroups)

table(rest$zipGroups, rest$zipCode)


#Easier cutting
library(Hmisc)
rest$zipGroups = cut2(rest$zipCode, g=4) #break them up according to the quantiles

table(rest$zipGroups)


#Create factor variables
rest$zcf <- factor(rest$zipCode)
rest$zcf[1:10]
class(rest$zcf)


#Levels of factor variables
yesno<- sample(c("yes", "no"), size=10, replace=TRUE)
yesnofac<- factor(yesno, levels= c("yes","no")) #default for levels is no
relevel(yesnofac, ref="yes")


#Mutate function: transform dataset 
library(Hmisc); library(plyr)
rest2 <- mutate(rest, zipGroups= cut2(zipCode,g=4))
table(rest2$zipGroups)

#common transforms

#abs(x) absolute value
#sqrt() square root
#ceiling 3.5 -->4
#floor 3.5 --> 3
#round(3.475,digits=2) is 3.48
#signif(3.475, digits=2) is 3.5


```


## **Data Reshaping**
```{r}
library(reshape2)
library(datasets)
head(mtcars)

#melt the dataset 
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id=c("carname","gear","cyl"), measure.vars=c("mpg","hp"))

head(carMelt, n=3)
tail(carMelt, n=3)


#Casting data frames
cylData <- dcast(carMelt, cyl~variable)# for cyl 4, there are 11 measurements for mpg..
cylData

cylData <- dcast(carMelt, cyl~variable,mean)
cylData


#Averaging values
head(InsectSprays)

#apply to count along the index spray, with sum
#sums up the count for each index spray
tapply(InsectSprays$count, InsectSprays$spray,sum)

#Another way using plyr package
ddply(InsectSprays, .(spray),summarise, sum=sum(count))
```



## **Intro to dplyr (IMPORTANT) **
```{r}
#Introduction to dplyr

library(dplyr)
chicago<- readRDS("/users/andrewhu/Documents/GitHub/Coursera-DataScienceSpecialization/03_Getting and Cleaning Data/Data/chicago.rds")
dim(chicago)
str(chicago)

names(chicago)

#look at subsets of columns
head(select(chicago, city:dptp))
head(select(chicago, -(city:dptp)))


#filter
#subset using multiple conditions
chic.f <- filter(chicago, pm25tmean2>30 & tmpd>80)
head(chic.f)

#arrange: reorder
chicago<- arrange(chicago,date)
head(chicago)
tail(chicago)

chicago<- arrange(chicago,desc(date))
head(chicago)


#rename (new name = old name)
chicago<- rename(chicago,pm25 = pm25tmean2, dewpoint=dptp)
head(chicago)

#Mutate:transform and create new var
chicago<- mutate(chicago, pm25detrend= pm25- mean(pm25,na.rm = TRUE))
head(chicago)


#group by: split a data frame according to categorical variables
chicago<- mutate(chicago, tempcat= factor(1* (tmpd>80), labels=c("cold","hot")))
hotcold<- group_by(chicago,tempcat)
hotcold

#summarize
summarize(hotcold, pm25=mean(pm25,na.rm=TRUE), o3=max(o3tmean2), no2= median(no2tmean2))


#summarize based on year
chicago<- mutate(chicago,year= as.POSIXlt(date)$year +1900)
years<- group_by(chicago,year)
summarize(years, pm25=mean(pm25,na.rm=TRUE), o3=max(o3tmean2), no2= median(no2tmean2))

#Pipeline operator
chicago %>% mutate(month = as.POSIXlt(date)$mon +1) %>% group_by(month) %>% summarize(pm25= mean(pm25, na.rm=TRUE), o3= max(o3tmean2), no2 = median(no2tmean2))
```


## ** Merge **
```{r}
#---------Merging data
review<- read.csv("/users/andrewhu/Documents/GitHub/Coursera-DataScienceSpecialization/03_Getting and Cleaning Data/Data/reviews.csv")
solu<- read.csv("/users/andrewhu/Documents/GitHub/Coursera-DataScienceSpecialization/03_Getting and Cleaning Data/Data/solutions.csv")

head(review)
head(solu)

names(review)
names(solu)

#merge by solution_id and id

mergedata= merge(review,solu,by.x="solution_id", by.y="id",all=TRUE)
head(mergedata)


#use plyr to merge

#e.g. arrange(join(df1,df2),id)


#Multiple dfs

df1 = data.frame(id=sample(1:10),x=rnorm(10))
df2 = data.frame(id=sample(1:10),y=rnorm(10))
df3 = data.frame(id=sample(1:10),z=rnorm(10))
dfList <- list(df1,df2,df3)
join_all(dfList)
```

---

## **Editing text**
```{r}
#Editing text var
camera<- fread("cameras.csv")
names(camera)

#make name lowercase
tolower(names(camera))


#fixing character vectors sub()

names(review)

sub("_","",names(review))
test<- "this_is_a_test"
sub("_","",test)#only edit the first encounter
gsub("_","",test)#edit all

#paste0: leave no space
paste0("Andrew","Hu")
```

## **Finding values in text**
```{r}
#Finding values in text
grep("Alameda", camera$intersection) #tells you the location
grep("Alameda", camera$intersection, value=TRUE) #return the value

#Subsetting with the finding values
camera2 <- camera[!grepl("Alameda",camera$intersection),]
```

##Regular Expressions
```
$ represent for the end of a line
morning$
^ will start looking for the begining of the line
```

## **Creating dates**
```{r}
x =c("1jan1960", "2jan1960","31mar1960","30jul1960"); z= as.Date(x, "%d%b%Y") #day, month, year
z

z[1]- z[2]
```
