---
title: "Week 3 Notes"
output: html_document
---
Content:

1. Predicting with trees
2. Bagging
3. Random Forests
4. Boosting
5. Model Based Prediction

---


## 1. Predicting with trees: Iris example
```{r iris, cache=TRUE}
data(iris); library(ggplot2);library(caret);library(rattle)
names(iris)
table(iris$Species)

#Model
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)

#Plot tree
plot(modFit$finalModel, uniform=TRUE, 
      main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)

#Prettier plots
library(rattle)
fancyRpartPlot(modFit$finalModel)

#Predict
predict(modFit,newdata=testing)
```

** Notes **

* Classification trees are non-linear models
  * They use interactions between variables
  * Data transformations may be less important (monotone transformations)
  * Trees can also be used for regression problems (continuous outcome)
  
---


## 2. Bagging

__Basic idea__: 

1. Resample cases and recalculate predictions
2. Average or majority vote

__Notes__:

* Similar bias 
* Reduced variance
* More useful for non-linear functions

* Bagging in caret ** 

* Some models perform bagging for you, in `train` function consider `method` options 
  * `bagEarth` 
  * `treebag`
  * `bagFDA`
* Alternatively you can bag any model you choose using the `bag` function

---

## More bagging in caret

```{r bag1}
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
                bagControl = bagControl(fit = ctreeBag$fit,
                                        predict = ctreeBag$pred,
                                        aggregate = ctreeBag$aggregate))
```


__Notes__:

* Bagging is most useful for nonlinear models
* Often used with trees - an extension is random forests
* Several models use bagging in caret's _train_ function


---


## 3. Random forests

1. Bootstrap samples
2. At each split, **bootstrap variables**
3. Grow multiple trees and vote

__Pros__:

1. **Accuracy**

__Cons__:

1. Speed
2. Interpretability
3. Overfitting

## Example:

```{r}
#loading libraries, data, and splitting data
data(iris); library(ggplot2); library(caret)
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

#Random forest in caret
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit

#take a look at one of the tree
getTree(modFit$finalModel,k=2)


## Predicting new values
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
```

---
# 4. Boosting

## Basic idea

1. Take lots of (possibly) weak predictors
2. Weight them and add them up
3. Get a stronger predictor


## Basic idea behind boosting

1. Start with a set of classifiers $h_1,\ldots,h_k$
  * Examples: All possible trees, all possible regression models, all possible cutoffs.
2. Create a classifier that combines classification


** Wage Example **

```{r wage, cache=TRUE}
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```

---

## Fit the model

```{r, dependson="wage", cache=TRUE}
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
```

## Plot the result
```{r}
qplot(predict(modFit,testing),wage,data=testing)
```

---

# 5. Model Based Approach
```{r pressure}
## Example: Iris Data
data(iris); library(ggplot2)
names(iris)
table(iris$Species)

## Create training and test sets


inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)

## Build predictions
modlda = train(Species ~ .,data=training,method="lda")
modnb = train(Species ~ ., data=training,method="nb")
plda = predict(modlda,testing); pnb = predict(modnb,testing)
table(plda,pnb)
```

## Comparison of results
```{r,dependson="fit",fig.height=4,fig.width=4}
equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
```
