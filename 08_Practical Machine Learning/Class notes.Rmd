---
title: 'Class notes: Practical Machine Learning'
output:
  pdf_document: default
  html_document: default
author: Andrew Abisha Hu
---
# **Table of content**

* **Week1**

        * Intro to machine learning
        
* **Week2: Caret Package**


        * Data splitting
                * createDataPartition
                * createResample
                * createTimeSlices
                * K-fold
                
        * preProcess
                * standardizing
                        * center and scale
                        * box-cox
                        * imputing data
                        
        * Covariate creation (features building)
                * dummy variables
                * Removing zero covariates
                
        * Splines
        
        * Preprocessing with Principal Components Analysis (PCA)
        
        * Predicting with regression
        
        * Data splitting
                * createDataPartition
                * createResample
                * createTimeSlices
                * K-fold
                
        * Training/testing functions
        
        * Model comparison
        
        * confusionMatrix
        
* **Week3: Trees **

        * Predicting with trees
        * Bagging
        * Random Forests
        * Boosting
        * Model Based Prediction
        
* **Week4**

        * Combine predictors

---

# **Week1**


## Process:

question -> input data -> features -> algorithm -> parameters -> evaluation 


**Features matter!**

__Properties of good features__

* Lead to data compression
* Retain relevant information
* Are created based on expert application knowledge

__Common mistakes__

* Trying to automate feature selection
* Not paying attention to data-specific quirks
* Throwing away information unnecessarily

**Algorithms matter less than you'd think**

**Issues to consider:**

Interpretable

> Simple

> Accurate

> Fast

> Scalable

## In sample and out sample error

__In Sample Error__: The error rate you get on the same
data set you used to build your predictor. Sometimes
called resubstitution error.

__Out of Sample Error__: The error rate you get on a new
data set. Sometimes called generalization error. 

__Key ideas__

1. Out of sample error is what you care about
2. In sample error $<$ out of sample error
3. The reason is overfitting
  * Matching your algorithm to the data you have
  
  
* Data have two parts
  * Signal
  * Noise
* The goal of a predictor is to find signal
* You can always design a perfect in-sample predictor
* You capture both signal + noise when you do that
* Predictor won't perform as well on new samples
  

 
## Prediction Study Design

1. Define your error rate
2. Split data into:
  * Training, Testing, Validation (optional)
3. On the training set pick features
  * Use cross-validation
4. On the training set pick prediction function
  * Use cross-validation
6. If no validation 
  * Apply 1x to test set
7. If validation
  * Apply to test set and refine
  * Apply 1x to validation 
  

 
[http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf](http://www2.research.att.com/~volinsky/papers/ASAStatComp.pdf) 


## Rules of thumb for prediction study design

* If you have a large sample size
  * 60% training
  * 20% test
  * 20% validation
* If you have a medium sample size
  * 60% training
  * 40% testing
* If you have a small sample size
  * Do cross validation
  * Report caveat of small sample size
  

## Basic terms

In general, __Positive__ = identified and __negative__ = rejected. Therefore:

__True positive__ = correctly identified

__False positive__ = incorrectly identified

__True negative__ = correctly rejected

__False negative__ = incorrectly rejected



** Summary **

![](/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Materials/error.jpg)

## For continuous data

__Mean squared error (MSE)__:

$$\frac{1}{n} \sum_{i=1}^n (Prediction_i - Truth_i)^2$$

__Root mean squared error (RMSE)__:

$$\sqrt{\frac{1}{n} \sum_{i=1}^n(Prediction_i - Truth_i)^2}$$
## Cross Validation:

![](/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Materials/CV.jpg)





**Key idea**

1. Accuracy on the training set (resubstitution accuracy) is optimistic
2. A better estimate comes from an independent set (test set accuracy)
3. But we can't use the test set when building the model or it becomes part of the training set
4. So we estimate the test set accuracy with the training set. 

**_Approach_:**

1. Use the training set

2. Split it into training/test sets 

3. Build a model on the training set

4. Evaluate on the test set

5. Repeat and average the estimated errors

**_Used for_:**

1. Picking variables to include in a model

2. Picking the type of prediction function to use

3. Picking the parameters in the prediction function

4. Comparing different predictors


## K-Fold

![](/Users/andrewhu/Documents/GitHub/Coursera_DataScience_JHU/Practical Machine Learning/Materials/kfold.jpg)

## Considerations

* For time series data, data must be used in "chunks"

* For k-fold cross validation
  * Larger k = less bias, more variance
  * Smaller k = more bias, less variance
  
* Random sampling must be done _without replacement_

* Random sampling with replacement is the _bootstrap_
  * Underestimates of the error
  * Can be corrected, but it is complicated ([0.632 Bootstrap](http://www.jstor.org/discover/10.2307/2965703?uid=2&uid=4&sid=21103054448997))
  
* If you cross-validate to pick predictors estimate you must estimate errors on independent data. 


---

# **Week 2**


---


# The Caret package Intro

* Some preprocessing (cleaning)
  * preProcess
        *Standardizing 
* Data splitting
  * createDataPartition
  * createResample
  * createTimeSlices
* Training/testing functions
  * train
  * predict
* Model comparison
  * confusionMatrix


## SPAM Example: Data splitting

```{r loadPackage}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE) #predict type, use 75% to train the model and 25% to test

#subset
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```

## SPAM Example: Fit a model

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm") #use all variables, and use "training" dataset to build the training model

modelFit
```


## SPAM Example: Final model

```{r finalModel, dependson="training",cache=TRUE}
modelFit <- train(type ~.,data=training, method="glm") 
modelFit$finalModel #check the actual fitted values that we got for our glm model
```

--- 

## SPAM Example: Prediction

```{r predictions, dependson="training",cache=TRUE}
predictions <- predict(modelFit,newdata=testing)
predictions
```

--- 

## SPAM Example: Confusion Matrix

```{r confusion, dependson="predictions",cache=TRUE}
confusionMatrix(predictions,testing$type)
```


# Data Slicing
```{r}
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE) #predict type, use 75% to train the model and 25% to test

#subset
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```


## SPAM Example: K-fold
```{r kfold,dependson="loadPackage"}
set.seed(32323) #set seed to have consistent result
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=TRUE)
sapply(folds,length)
folds[[1]][1:10]
```


## SPAM Example: Return test

```{r kfoldtest,dependson="loadPackage"}
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=FALSE)
sapply(folds,length)
folds[[1]][1:10]
```

---

## SPAM Example: Resampling

```{r resample,dependson="loadPackage"}
set.seed(32323)
folds <- createResample(y=spam$type,times=10,
                             list=TRUE)
sapply(folds,length)
folds[[1]][1:10]
```

## SPAM Example: Time Slices 

```{r time,dependson="loadPackage"}
set.seed(32323)
tme <- 1:1000 #for time series data
folds <- createTimeSlices(y=tme,initialWindow=20,
                          horizon=10)
names(folds)
folds$train[[1]]
folds$test[[1]]
```


# Plotting predictors

## Example: Wage data

```{r loadData,cache=TRUE}
library(ISLR); library(ggplot2); library(caret); library(gridExtra);
data(Wage)
summary(Wage)
```


---

## Get training/test sets

```{r trainingTest,dependson="loadData",cache=TRUE}
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```


---

## Feature plot (*caret* package)

```{r ,dependson="trainingTest",fig.height=4,fig.width=4}
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
#reveal the relationship between each variables
```


---

## Qplot (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,data=training) #simple scatter plot
```


---

## Qplot with color (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(age,wage,colour=jobclass,data=training) #scatter plot separated by jobclass, indicating by different colors
```


---

## Add regression smoothers (*ggplot2* package)


```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qq <- qplot(age,wage,colour=education,data=training)
qq +  geom_smooth(method='lm',formula=y~x)
#separated by education, and add regression line
```


---

## cut2, making factors (*Hmisc* package)


```{r cut2,dependson="trainingTest",fig.height=4,fig.width=6,cache=TRUE}
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
#smart cut for 3 intervals
```

---

## Boxplots with three groups we just created

```{r ,dependson="cut2plot",fig.height=4,fig.width=6,cache=TRUE}
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot"))
p1
```

---

## Boxplots with points overlayed

```{r ,dependson="cut2plot",fig.height=4,fig.width=9}
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot","jitter")) #boxplot with points
grid.arrange(p1,p2,ncol=2) #plotting two graphs together
```


---

## Tables

```{r ,dependson="cut2",fig.height=4,fig.width=9}
t1 <- table(cutWage,training$jobclass) 
t1  #lower wage tends to have more industrial jobs
prop.table(t1,1)
```


---

## Density plots

```{r ,dependson="trainingTest",fig.height=4,fig.width=6}
qplot(wage,colour=education,data=training,geom="density")
```

---

## Final Notes

* Make your plots only in the training set 
  * Don't use the test set for exploration!
* Things you should be looking for
  * Imbalance in outcomes/predictors
  * Outliers 
  * Groups of points not explained by a predictor
  * Skewed variables 
  
---


  
# Pre-Processing


Preview the data: Skewed??
```{r}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
```


**Purpose: Avoid to be tricked by large variation**


## Standardizing

**Approach 1: Take the value, subtract the mean, and divided by sd** 
**Note: might not be useful. There are other methods**
```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
#Training Set

trainCapAve <- training$capitalAve

trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve)  # remember to use mean and sd from the training set

mean(trainCapAveS) #will be zero
sd(trainCapAveS) # will be one

#Test Set

testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve)  # remember to use mean and sd from the training set
mean(testCapAveS) # will not exactly be zero
sd(testCapAveS) # will not exactly be one
```

**Approach 2: Using the preProcess function in the caret package, centering and scaling **

```{r}
preObj <- preProcess(training[,-58],method=c("center","scale")) #include all of our variables except the outcome

#Training set

trainCapAveS <-
predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS) #should be zero
sd(trainCapAveS) # should be one

#Test set

testCapAveS <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)

```

**Approach 3:  Standardizing - _preProcess_ argument**

```{r training, dependson="loadPackage",cache=TRUE}
set.seed(32343)
modelFit <- train(type ~.,data=training,
                  preProcess=c("center","scale"),method="glm")
modelFit
```


**Approach 4: Box-Cox transforms**

```{r ,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58]$capitalAve)
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
```


**Apporach 5: Imputing data**

-- Common way to deal with **missing data**
-- If you have some missing data, you can impute them using k-nearest neighbor's impuration

```{r knn,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=7}
set.seed(13343)

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1 #generate some random data
training$capAve[selectNA] <- NA #set them to NA

# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58]$capAve)

# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
```

## Final Note:

** Training and test must be processed in the same way

** Test transformations will be imperfect-- that's fine

---


# **Covariate creation (features building)**


## Level 1, Raw data -> covariates

* Examples:
  * Text files: frequency of words, frequency of phrases, frequency of capital letters.
  * Images: Edges, corners, blobs, ridges 
  * Webpages: Number and type of images, position of elements, colors, videos
  * People: Height, weight, hair color, sex, country of origin. 
  
  
* The more knowledge of the system you have the better the job you will do. 
* When in doubt, err on the side of more features
* Can be automated, but use caution!


## Level 2, Tidy covariates -> new covariates (Transformations or functions of the covariates)

* More necessary for some methods (regression, svms) than for others (classification trees).
* Should be done **only on the training set**
* The best approach is through exploratory analysis (plotting/tables)
* New covariates should be added to data frames



## Load example data
```{r loadData,cache=TRUE}
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```


---

## Common covariates to add, **dummy variables**

```{r dummyVar,dependson="loadData"}
table(training$jobclass) #at first, it's a character
dummies <- dummyVars(wage ~ jobclass,data=training) #create dummy in caret package
head(predict(dummies,newdata=training))
```


## Removing zero covariates

Note: Some of the variables are basically have no variability in them, for example, an email will always have words (it won't be zero). So we can use the near zero function in caret package to identify the variables which will not likely be good predictors

```{r ,dependson="dummyVar"}
nsv <- nearZeroVar(training,saveMetrics=TRUE) #save the metrics to see how it is calculating
nsv
```


Note: from the above, we can see the percentage of unique values for a particular variable. E.g. Year has about 0.33% of unique values, and it's not a near zero variable. But, for region, it has a very low frequency ratio. It is basically a one category and a near zero variable. Hence, you can exclude that.


---

## Spline basis: Fit a curve line

```{r splines,dependson="dummyVar",cache=TRUE}
library(splines)
bsBasis <- bs(training$age,df=3) 
bsBasis
```


---

## Fitting curves with splines

```{r ,dependson="splines",fig.height=4,fig.width=4}
lm1 <- lm(wage ~ bsBasis,data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
```


---

## Splines on the test set

```{r ,dependson="splines",fig.height=4,fig.width=4}
predict(bsBasis,age=testing$age) # use the same procedure with 
```

---



# Preprocessing with Principal Components Analysis (PCA)

## Basic PCA idea

* Dealing with corelated predictors

* You need to include variables that captures most of the information
* We might not need every predictor
* A weighted combination of predictors might be better
* We should pick this combination to capture the "most information" possible

* Benefits
  * Reduced number of predictors
  * Reduced noise (due to averaging)
  * Reduce the size of datasets but still catch most of the variation


## First step: Checking Correlated predictors and Splitting Data
```{r loadPackage,cache=TRUE,fig.height=3.5,fig.width=3.5}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

M <- abs(cor(training[,-58]))
diag(M) <- 0 # set up the correlation of the variable and itself to be zero
which(M > 0.8,arr.ind=T) #showing which variables has a correlation higher than 0.8
```


## Plotting: Correlated predictors
```{r,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
```

  
# Figure out how to do PCA


## Principal components in R - prcomp(basic stats package)
```{r prcomp,dependson="loadPackage",cache=TRUE,fig.height=3.5,fig.width=3.5}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
```

---

## Principal components in R - prcomp rotation table
```{r}
prComp$rotation #check how these variables sum up
```


## PCA on SPAM data: Exercise
```{r}
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1)) #PCA on the entire data set
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
```

---

#  **PCA with caret (Important):**


The PC in the whole spam data set and plotting.
```{r}
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2) # assign methods and numbers of components to compute
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
```


## Preprocessing with PCA: Completed process elaboration
```{r}
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2) #Pre-pcoess with th traiing data and exclude the outcome, set pca variables to 2
trainPC <- predict(preProc,log10(training[,-58]+1)) #using the preProcess model to predict the training data
modelFit <- train(training$type ~ .,method="glm",data=trainPC) 
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
```

---

## **Alternative (sets # of PCs): More convinient, recommended!!** 

**How to build a pca model in the most efficient way using Caret**
```{r}
modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))
```



## Final Note:

1. It is most useful for linear-type models
2. May make it harder to interpret predictors
3. Watch out for outliers
 - Transform your variables first (log10 / Box Cox)
 - Plot predictors first to identify potential problems
 
 
---

# Predicting with regression 

## Key ideas

* Fit a simple regression model
* Plug in new covariates and multiply by the coefficients
* Useful when the linear model is (nearly) correct

__Pros__:
* Easy to implement
* Easy to interpret

__Cons__:
* Often poor performance in nonlinear settings


## Example: Old faithful eruptions

```{r}
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
                              p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith)
```

## Fit a linear model 

$$ ED_i = b_0 + b_1 WT_i + e_i $$

```{r}
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)
```


## Get training set/test set errors

```{r }
# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))

# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))

```


## Prediction intervals

```{r}
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",col=c(1,2,2),lty = c(1,1,1), lwd=3)
```


## **Same process with caret**

```{r}
modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
summary(modFit$finalModel) #same with our SLR
```

---


# Predicting with regression, multiple covariates (A Summary)

## Example: Wage data

**1. Loading libraries and subsetting the data **
```{r}
library(ISLR); library(ggplot2); library(caret);
data(Wage) 
Wage <- subset(Wage,select=-c(logwage)) #duplicate
summary(Wage)
```


**2. Get training/test sets**
```{r}
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
dim(training); dim(testing)
```



## **3. Exploratory Analysis**

3-1: Feature plot: the plot of showing relationship between each variables
```{r}
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
```

3-2: Plotting with different variables

## Plot age versus wage
```{r}
qplot(age,wage,data=training)
```


## Plot age versus wage colour by jobclass
```{r}
qplot(age,wage,colour=jobclass,data=training)
```


## Plot age versus wage colour by education
```{r}
qplot(age,wage,colour=education,data=training)
```


## **4.Fit a linear model** 

$$ ED_i = b_0 + b_1 age + b_2 I(Jobclass_i="Information") + \sum_{k=1}^4 \gamma_k I(education_i= level k) $$

```{r}
modFit<- train(wage ~ age + jobclass + education,
               method = "lm",data=training)
finMod <- modFit$finalModel
print(modFit)
```

## **5. Diagnostics **

```{r}
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
```


---

## Color by variables not used in the model 
```{r}
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
```

---

## Plot by index
```{r}
plot(finMod$residuals,pch=19)
```

## Predicted versus truth in test set
```{r }
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
```

---

## Include all covariates
```{r }
modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)
```




---



Quiz:

Load the Alzheimer's disease data using the commands:

```
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]

```
Find all the predictor variables in the training set that begin with IL. Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?


```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
```



Load the Alzheimer's disease data using the commands:


```
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
```
Create a training data set consisting of only the predictors with variable names beginning with IL and the diagnosis. Build two predictive models, one using the predictors as they are and one using PCA with principal components **explaining 80% of the variance in the predictors**. Use method="glm" in the train function.

What is the accuracy of each method in the test set? Which is more accurate?

```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

IL_str <- grep("^IL", colnames(training), value = TRUE)

training <- adData[IL_str,]
testing <- adData[-IL_str,]


## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]

#Non-PCA model
modelFit1 <- train(diagnosis ~ ., method = "glm", data = training) #no pre-process
pd1<- predict(modelFit1, testing) #use training model to predict testing set
confusionMatrix(pd1,testing$diagnosis) #check for accuracy

#PCA model
modelFit2 <- train(diagnosis ~ ., method = "glm", preProcess = "pca", 
    data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
confusionMatrix(predict(modelFit2, testing),testing$diagnosis)
```

---

# **Week3**

---


## 1. Predicting with trees: Iris example
```{r iris, cache=TRUE}
data(iris); library(ggplot2);library(caret);library(rattle)
names(iris)
table(iris$Species)

#Model
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)

#Plot tree
plot(modFit$finalModel, uniform=TRUE, 
      main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)

#Prettier plots
library(rattle)
fancyRpartPlot(modFit$finalModel)

#Predict
predict(modFit,newdata=testing)
```

** Notes **

* Classification trees are non-linear models
  * They use interactions between variables
  * Data transformations may be less important (monotone transformations)
  * Trees can also be used for regression problems (continuous outcome)
  
---


## 2. Bagging

__Basic idea__: 

1. Resample cases and recalculate predictions
2. Average or majority vote

__Notes__:

* Similar bias 
* Reduced variance
* More useful for non-linear functions

* Bagging in caret ** 

* Some models perform bagging for you, in `train` function consider `method` options 
  * `bagEarth` 
  * `treebag`
  * `bagFDA`
* Alternatively you can bag any model you choose using the `bag` function

---

## More bagging in caret

```{r bag1}
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
                bagControl = bagControl(fit = ctreeBag$fit,
                                        predict = ctreeBag$pred,
                                        aggregate = ctreeBag$aggregate))
```


__Notes__:

* Bagging is most useful for nonlinear models
* Often used with trees - an extension is random forests
* Several models use bagging in caret's _train_ function


---


## 3. Random forests

1. Bootstrap samples
2. At each split, **bootstrap variables**
3. Grow multiple trees and vote

__Pros__:

1. **Accuracy**

__Cons__:

1. Speed
2. Interpretability
3. Overfitting

## Example:

```{r}
#loading libraries, data, and splitting data
data(iris); library(ggplot2); library(caret)
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

#Random forest in caret
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit

#take a look at one of the tree
getTree(modFit$finalModel,k=2)


## Predicting new values
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
```

---
# 4. Boosting

## Basic idea

1. Take lots of (possibly) weak predictors
2. Weight them and add them up
3. Get a stronger predictor


## Basic idea behind boosting

1. Start with a set of classifiers $h_1,\ldots,h_k$
  * Examples: All possible trees, all possible regression models, all possible cutoffs.
2. Create a classifier that combines classification


** Wage Example **

```{r wage, cache=TRUE}
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```

---

## Fit the model

```{r, dependson="wage", cache=TRUE}
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
```

## Plot the result
```{r}
qplot(predict(modFit,testing),wage,data=testing)
```

---

# 5. Model Based Approach
```{r pressure}
## Example: Iris Data
data(iris); library(ggplot2)
names(iris)
table(iris$Species)

## Create training and test sets


inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)

## Build predictions
modlda = train(Species ~ .,data=training,method="lda")
modnb = train(Species ~ ., data=training,method="nb")
plda = predict(modlda,testing); pnb = predict(modnb,testing)
table(plda,pnb)
```

## Comparison of results
```{r,dependson="fit",fig.height=4,fig.width=4}
equalPredictions = (plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
```

---

# **Week 4**

## Combining Predictors

## Key ideas

* You can combine classifiers by averaging/voting
* Combining classifiers improves accuracy
* Combining classifiers reduces interpretability
* Boosting, bagging, and random forests are variants on this theme

## Example

__Create training, test and validation sets__
```{r wage}
library(ISLR); data(Wage); library(ggplot2); library(caret);


# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]

inTrain <- createDataPartition(y=buildData$wage,
                              p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
```

__ Build two different models, glm and randomForest __

```{r modFit}
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
              data=training, 
              trControl = trainControl(method="cv"),number=3)
```

__ Predict on the testing set using the models we built __

```{r predict}
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
```


__ Fit a model that combines predictors __

```{r combine,dependson="predict"}
predDF <- data.frame(pred1,pred2,wage=testing$wage) #Create a data frame combining pred1 and 2, and the outcome from testing set --> "Combining data set"
combModFit <- train(wage ~.,method="gam",data=predDF) #use "gam" to fit the combining data set
combPred <- predict(combModFit,predDF) #use the gam model to predict the combining data set
```


__ Testing errors __

```{r ,dependson="combine"}
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
```


---

__ Predict on validation data set : For cross validation __

```{r validation,dependson="combine"}
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
```


---

__ Evaluate on validation __

```{r ,dependson="validation"}
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2)) # the error is lower. Legit.
```

---